{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C34e9E-5iuei"
   },
   "source": [
    "# Causally Denoise Word Embeddings Using Half-Sibling Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOBoxB0yi1wl"
   },
   "source": [
    "## Load word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20033,
     "status": "ok",
     "timestamp": 1566550265179,
     "user": {
      "displayName": "Zekun Yang",
      "photoUrl": "",
      "userId": "00530960366744250949"
     },
     "user_tz": -480
    },
    "id": "_hrCETrJip8A",
    "outputId": "558ec7af-c657-4b44-b8bb-b257db9c7125"
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "import codecs\n",
    "import numpy as np\n",
    "import nltk\n",
    "from heapq import nlargest\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import spearmanr\n",
    "import os, csv, re, requests, scipy\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import functools as ft\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "juyQDw-kK7Wn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1iLd0Wz0bVazXvJiGZvEJ67QCgcj4DMCM\n",
      "To: E:\\CityU Spring 2019\\Research\\word-vector-NLP\\denoise\\Code Causally Denoise Word Embeddings Using Half-Sibling Regression\\data\\small_word2vec.txt\n",
      "400MB [07:20, 909kB/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/small_word2vec.txt'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download Word2Vec\n",
    "url = 'https://drive.google.com/uc?id=1iLd0Wz0bVazXvJiGZvEJ67QCgcj4DMCM'\n",
    "output = 'data/small_word2vec.txt'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O0YnROKRiJAD"
   },
   "outputs": [],
   "source": [
    "def loadWordVecs(model_str):\n",
    "    word_dictionary = {}\n",
    "    \n",
    "    input_file_destination = 'data/small_' + model_str + '.txt'\n",
    "\n",
    "    f = codecs.open(input_file_destination, 'r', 'utf-8') \n",
    "\n",
    "    for line in f:\n",
    "\n",
    "        line = line.split(\" \", 1)   \n",
    "        transformed_key = line[0].lower()\n",
    "\n",
    "        try:\n",
    "            transformed_key = str(transformed_key)\n",
    "\n",
    "        except:\n",
    "            print(\"Can't convert the key to unicode:\", transformed_key)\n",
    "\n",
    "        word_dictionary[transformed_key] = np.fromstring(line[1], dtype=\"float32\", sep=\" \")\n",
    "\n",
    "        if word_dictionary[transformed_key].shape[0] != 300:\n",
    "            print(transformed_key, word_dictionary[transformed_key].shape)\n",
    "\n",
    "    return  word_dictionary     \n",
    "\n",
    "orig_word2vec = loadWordVecs('word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vh14CY4Cmpiu"
   },
   "outputs": [],
   "source": [
    "orig_model = {}\n",
    "\n",
    "orig_model['word2vec'] = orig_word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WdSLrV36i5K1"
   },
   "source": [
    "## Load stop words (function words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1196,
     "status": "ok",
     "timestamp": 1566550065546,
     "user": {
      "displayName": "Zekun Yang",
      "photoUrl": "",
      "userId": "00530960366744250949"
     },
     "user_tz": -480
    },
    "id": "Rb8fzeblmE2T",
    "outputId": "17f6f18f-df5f-446f-b7f0-2c1fda3fe111"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ZKY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# all stop words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "STOP = list(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "# all nonstop words\n",
    "nonStop = list(set(orig_word2vec.keys() ) - set(STOP)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kpzsOq09mL_U"
   },
   "outputs": [],
   "source": [
    "def ensemble_wordvec_mat(wordVecModel_str, wordList, orig_model = orig_model):\n",
    "    \n",
    "    # put the word vectors in columns\n",
    "    feasibleWordList = list(set(orig_model[wordVecModel_str].keys() ) & set(wordList)) \n",
    "        \n",
    "    x_collector = []\n",
    "    newDict = {}\n",
    "    for word in feasibleWordList:\n",
    "        x_collector.append(orig_model[wordVecModel_str][word])\n",
    "        newDict[word] = orig_model[wordVecModel_str][word][:]        \n",
    "                        \n",
    "    x_collector = np.array(x_collector).T    \n",
    "    \n",
    "    return newDict, x_collector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9i_o4HnbmiFy"
   },
   "outputs": [],
   "source": [
    "# emsemble dictionary for stop words and non stop words\n",
    "StopWordDict, StopWordVecs = ensemble_wordvec_mat('word2vec', STOP)\n",
    "nonStopWordDict, nonStopWordVecs = ensemble_wordvec_mat('word2vec', nonStop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XqclRz9am7kX"
   },
   "source": [
    "# Half-Sibling Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeEJldw7m6nj"
   },
   "outputs": [],
   "source": [
    "def HSR_RR(InputVec,TargetVec,TargetDict):\n",
    "    alpha = 50 # ridge regression parameter\n",
    "    \n",
    "    W = np.linalg.inv(InputVec.T @ InputVec + alpha * np.eye(InputVec.shape[1])) @ InputVec.T @ TargetVec\n",
    "    W = np.array(W)\n",
    "    post_TargetVec = TargetVec  - InputVec @ W # modify those non-stop words\n",
    "    \n",
    "    post_TargetDict = TargetDict.copy() # copy the dictionary of non-stop words\n",
    "\n",
    "    i = 0\n",
    "    for w in TargetDict.keys():\n",
    "        post_TargetDict[w] = post_TargetVec[:, i] # update the modified non-stop words\n",
    "        i += 1\n",
    "  \n",
    "    return post_TargetDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8UCevZIdof25"
   },
   "source": [
    "## Carry out half-sibling regression for content-word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RQn-U4ZvoQfB"
   },
   "outputs": [],
   "source": [
    "post_nonStopWordDict = HSR_RR(StopWordVecs,nonStopWordVecs,nonStopWordDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SwAymtPYo32w"
   },
   "source": [
    "## Carry out half-sibling regression for stop-word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xvCnxUFBqCsT"
   },
   "outputs": [],
   "source": [
    "# We use some content-word vectors to predict stop-word vectors. To this end, we first extract commonly used content words. \n",
    "wikiWordsPath = 'data/enwiki_vocab_min200.txt' # This file can be downloaded froom https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
    "wikiWords = {}\n",
    "\n",
    "with open(wikiWordsPath, \"r+\") as f_in:\n",
    "    for line in f_in:\n",
    "        wikiWords[line.split(' ')[0]] = int(line.split(' ')[1])\n",
    "\n",
    "freq_content_word = list(set(wikiWords.keys()) & set(orig_word2vec.keys()))\n",
    "non_stop_freq_content_word = list(set(freq_content_word) - set(STOP))\n",
    "\n",
    "wikiWords_nsfc = {}\n",
    "for word in non_stop_freq_content_word:\n",
    "     wikiWords_nsfc[word] = wikiWords[word]\n",
    "\n",
    "feature_nonStop = nlargest(1000, wikiWords_nsfc, key=wikiWords_nsfc.get)\n",
    " \n",
    "nonStopWordVecs_features = np.array([nonStopWordDict[word] for word in feature_nonStop]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jQVXA0MDouY7"
   },
   "outputs": [],
   "source": [
    "post_StopWordDict = HSR_RR(nonStopWordVecs_features,StopWordVecs,StopWordDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4ZZkQWDpBKj"
   },
   "outputs": [],
   "source": [
    "post_word2vec = {**post_nonStopWordDict, **post_StopWordDict} # merge stop and non-stop word vectors into a single dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1062,
     "status": "ok",
     "timestamp": 1566551094334,
     "user": {
      "displayName": "Zekun Yang",
      "photoUrl": "",
      "userId": "00530960366744250949"
     },
     "user_tz": -480
    },
    "id": "wFavUiL-pGHJ",
    "outputId": "6e8869e3-2f1e-4335-abef-e1744b91109c"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8DSuqooMuDfK"
   },
   "source": [
    "## Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSets = ['EN-RG-65.txt', 'EN-WS-353-ALL.txt', 'EN-RW-STANFORD.txt', 'EN-MEN-TR-3k.txt', 'EN-MTurk-287.txt', 'EN-SIMLEX-999.txt', 'EN-SimVerb-3500.txt']\n",
    "\n",
    "\n",
    "def similarity_eval(dataSetAddress, wordVecModel_str):\n",
    "    wordVecModel = eval(wordVecModel_str)\n",
    "    vocab = set(list(wordVecModel.keys()))\n",
    "    \n",
    "    fread_simlex = open(dataSetAddress, \"r\")\n",
    "    \n",
    "    pair_list = []\n",
    "\n",
    "    line_number = 0\n",
    "    for line in fread_simlex:\n",
    "#         if line_number > 0:\n",
    "        tokens = line.split()\n",
    "        word_i = tokens[0]\n",
    "        word_j = tokens[1]\n",
    "        score = float(tokens[2])\n",
    "        if word_i in vocab and word_j in vocab:\n",
    "            pair_list.append( ((word_i, word_j), score) )\n",
    "#         line_number += 1\n",
    "\n",
    "    pair_list.sort(key=lambda x: - x[1]) # order the pairs from highest score (most similar) to lowest score (least similar)\n",
    "\n",
    "\n",
    "    extracted_scores = {}\n",
    "\n",
    "    extracted_list = []\n",
    "    \n",
    "               \n",
    "    for (x,y) in pair_list:\n",
    "        (word_i, word_j) = x\n",
    "        \n",
    "        current_distance = 1- cosine_similarity( wordVecModel[word_i].reshape(1,-1)  , wordVecModel[word_j].reshape(1,-1) )        \n",
    "\n",
    "        extracted_scores[(word_i, word_j)] = current_distance\n",
    "        extracted_list.append(((word_i, word_j), current_distance))\n",
    "\n",
    "    extracted_list.sort(key=lambda x: x[1])\n",
    "\n",
    "    spearman_original_list = []\n",
    "    spearman_target_list = []\n",
    "\n",
    "    for position_1, (word_pair, score_1) in enumerate(pair_list):\n",
    "        score_2 = extracted_scores[word_pair]\n",
    "        position_2 = extracted_list.index((word_pair, score_2))\n",
    "        spearman_original_list.append(position_1)\n",
    "        spearman_target_list.append(position_2)\n",
    "\n",
    "    spearman_rho = spearmanr(spearman_original_list, spearman_target_list)\n",
    "    \n",
    "    return spearman_rho[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating the data set EN-RG-65.txt\n",
      "word2vec + StopWordPost : 0.7569\n",
      "word2vec + Orig : 0.7494\n",
      "\n",
      "\n",
      "evaluating the data set EN-WS-353-ALL.txt\n",
      "word2vec + StopWordPost : 0.7059\n",
      "word2vec + Orig : 0.6999\n",
      "\n",
      "\n",
      "evaluating the data set EN-RW-STANFORD.txt\n",
      "word2vec + StopWordPost : 0.6033\n",
      "word2vec + Orig : 0.5997\n",
      "\n",
      "\n",
      "evaluating the data set EN-MEN-TR-3k.txt\n",
      "word2vec + StopWordPost : 0.7726\n",
      "word2vec + Orig : 0.7706\n",
      "\n",
      "\n",
      "evaluating the data set EN-MTurk-287.txt\n",
      "word2vec + StopWordPost : 0.6854\n",
      "word2vec + Orig : 0.6831\n",
      "\n",
      "\n",
      "evaluating the data set EN-SIMLEX-999.txt\n",
      "word2vec + StopWordPost : 0.4672\n",
      "word2vec + Orig : 0.4427\n",
      "\n",
      "\n",
      "evaluating the data set EN-SimVerb-3500.txt\n",
      "word2vec + StopWordPost : 0.3978\n",
      "word2vec + Orig : 0.3659\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in dataSets:\n",
    "    dataSetAddress = 'data/wordSimData/' +  dataset\n",
    "    print('evaluating the data set', dataset)\n",
    "    print('word2vec + StopWordPost : %.4f' %  similarity_eval(dataSetAddress, 'post_word2vec'))\n",
    "    print('word2vec + Orig : %.4f' %  similarity_eval(dataSetAddress, 'orig_word2vec'))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Textual Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sts_dataset(filename):\n",
    "    # For a STS dataset, loads the relevant information: the sentences and their human rated similarity score.\n",
    "    sent_pairs = []\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            ts = line.strip().split(\"\\t\")\n",
    "            if len(ts) == 7 or len(ts) == 9:\n",
    "                sent_pairs.append((re.sub(\"[^0-9]\", \"\", ts[2]) + '-' + ts[1] , ts[5], ts[6], float(ts[4])))\n",
    "            elif len(ts) == 6 or len(ts) == 8:\n",
    "                sent_pairs.append((re.sub(\"[^0-9]\", \"\", ts[1]) + '-' + ts[0] , ts[4], ts[5], float(ts[3])))\n",
    "            else:\n",
    "                print('data format is wrong!!!')\n",
    "    return pd.DataFrame(sent_pairs, columns=[\"year-task\", \"sent_1\", \"sent_2\", \"sim\"])\n",
    "\n",
    "\n",
    "def load_all_sts_dataset():\n",
    "    # Loads all of the STS datasets \n",
    "    stsbenchmarkDir = 'data/stsbenchmark/'\n",
    "    stscompanionDir = 'data/stsbenchmark/'\n",
    "    sts_train = load_sts_dataset(os.path.join(stsbenchmarkDir, \"sts-train.csv\"))    \n",
    "    sts_dev = load_sts_dataset(os.path.join(stsbenchmarkDir, \"sts-dev.csv\"))\n",
    "    sts_test = load_sts_dataset(os.path.join(stsbenchmarkDir, \"sts-test.csv\"))\n",
    "    sts_other = load_sts_dataset(os.path.join(stscompanionDir, \"sts-other.csv\"))\n",
    "    sts_mt = load_sts_dataset(os.path.join(stscompanionDir, \"sts-mt.csv\"))\n",
    "    \n",
    "    sts_all = pd.concat([sts_train, sts_dev, sts_test, sts_other, sts_mt ])\n",
    "    \n",
    "    return sts_all\n",
    "\n",
    "sts_all = load_all_sts_dataset()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_sts_by_year_task():\n",
    "    # Divide STS datasets based on their year and tasks\n",
    "    sts_by_year_task = {}\n",
    "    \n",
    "    for year_task in sts_all['year-task'].unique():\n",
    "        indices = [i for i, x in enumerate(list(sts_all['year-task'])) if x == year_task]\n",
    "        \n",
    "        pairs = sts_all.iloc[indices]\n",
    "        \n",
    "        sts_by_year_task[year_task] = pairs\n",
    "        \n",
    "    return sts_by_year_task\n",
    "\n",
    "sts_by_year_task = load_sts_by_year_task()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_sts_by_year():\n",
    "    # Divide STS datasets ONLY based on their year (different tasks in that year are merged).\n",
    "\n",
    "    sts_by_year = {}\n",
    "    \n",
    "    for year in ['2012', '2013', '2014', '2015', '2016', '2017']:\n",
    "        indices = [i for i, x in enumerate(list(sts_all['year-task'])) if x.startswith(year)]\n",
    "        \n",
    "        pairs = sts_all.iloc[indices]\n",
    "        pairs = pairs.copy()\n",
    "        pairs['year-task'] = year\n",
    "        sts_by_year[year] = pairs\n",
    "        \n",
    "    return sts_by_year\n",
    "\n",
    "sts_by_year_task = load_sts_by_year_task()\n",
    "\n",
    "sts_by_year = load_sts_by_year()\n",
    "\n",
    "filename = 'data/stsbenchmark/2015-answers-students.test.tsv'\n",
    "sent_pairs = []\n",
    "with tf.gfile.GFile(filename, \"r\") as f:\n",
    "    for line in f:\n",
    "        ts = line.strip().split(\"\\t\")\n",
    "        if len(ts) == 3:\n",
    "            sent_pairs.append((ts[1], ts[2], float(ts[0])))\n",
    "answers_students_2015 =  pd.DataFrame(sent_pairs, columns=[\"sent_1\", \"sent_2\", \"sim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sick(f): \n",
    "\n",
    "    response = requests.get(f).text\n",
    "\n",
    "    lines = response.split(\"\\n\")[1:]\n",
    "    lines = [l.split(\"\\t\") for l in lines if len(l) > 0]\n",
    "    lines = [l for l in lines if len(l) == 5]\n",
    "\n",
    "    df = pd.DataFrame(lines, columns=[\"idx\", \"sent_1\", \"sent_2\", \"sim\", \"label\"])\n",
    "    df['sim'] = pd.to_numeric(df['sim'])\n",
    "    return df\n",
    "    \n",
    "sick_all = download_sick(\"https://raw.githubusercontent.com/alvations/stasis/master/SICK-data/SICK_test_annotated.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    \n",
    "    def __init__(self, sentence):\n",
    "        self.raw = sentence\n",
    "        normalized_sentence = sentence.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "        self.tokens = [t.lower() for t in nltk.word_tokenize(normalized_sentence)]\n",
    "        \n",
    "def run_benchmark(sentences1, sentences2, model_str): \n",
    "    \n",
    "    model = eval(model_str)\n",
    "    embeddings = []\n",
    "\n",
    "    for (sent1, sent2) in zip(sentences1, sentences2): \n",
    "\n",
    "        tokens1 =  sent1.tokens\n",
    "        tokens2 =  sent2.tokens\n",
    "\n",
    "        tokens1 = [token for token in tokens1 if token in model and token.islower()]\n",
    "        tokens2 = [token for token in tokens2 if token in model and token.islower()]\n",
    "        \n",
    "        if tokens1 == [] and tokens2 != []:\n",
    "            embedding1 = np.zeros(300)\n",
    "            embedding2 = np.average([model[token] for token in tokens2], axis=0)\n",
    "        elif tokens2 == [] and tokens1 != []:\n",
    "            embedding2 = np.zeros(300)\n",
    "            embedding1 = np.average([model[token] for token in tokens1], axis=0)\n",
    "        elif tokens2 != [] and tokens1 != []:     \n",
    "            embedding1 = np.average([model[token] for token in tokens1], axis=0)\n",
    "            embedding2 = np.average([model[token] for token in tokens2], axis=0)\n",
    "        else:\n",
    "            embedding1 = np.zeros(300)\n",
    "            embedding2 = np.zeros(300)\n",
    "\n",
    "\n",
    "        embeddings.append(embedding1)\n",
    "        embeddings.append(embedding2)\n",
    "\n",
    "\n",
    "    sims = [cosine_similarity(embeddings[idx*2].reshape(1, -1), embeddings[idx*2+1].reshape(1, -1))[0][0] for idx in range(int(len(embeddings)/2))]\n",
    "    return sims\n",
    "\n",
    "def run_experiment(df, benchmarks): \n",
    "    \n",
    "    sentences1 = [Sentence(s) for s in df['sent_1']]\n",
    "    sentences2 = [Sentence(s) for s in df['sent_2']]\n",
    "    \n",
    "    pearson_cors, spearman_cors = [], []\n",
    "    for label, method in benchmarks:\n",
    "        sims = method(sentences1, sentences2)\n",
    "        pearson_correlation = round(scipy.stats.pearsonr(sims, df['sim'])[0] * 100,2)\n",
    "        pearson_cors.append(pearson_correlation)\n",
    "        \n",
    "    return pearson_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS-2012-MSRvid\n",
      "STS-2014-images\n",
      "STS-2015-images\n",
      "STS-2014-deft-forum\n",
      "STS-2012-MSRpar\n",
      "STS-2014-deft-news\n",
      "STS-2013-headlines\n",
      "STS-2014-headlines\n",
      "STS-2015-headlines\n",
      "STS-2016-headlines\n",
      "STS-2017-track5.en-en\n",
      "STS-2015-answers-forums\n",
      "STS-2016-answer-answer\n",
      "STS-2012-surprise.OnWN\n",
      "STS-2013-FNWN\n",
      "STS-2013-OnWN\n",
      "STS-2014-OnWN\n",
      "STS-2014-tweet-news\n",
      "STS-2015-belief\n",
      "STS-2016-plagiarism\n",
      "STS-2016-question-question\n",
      "STS-2012-SMTeuroparl\n",
      "STS-2012-surprise.SMTnews\n",
      "STS-2016-postediting\n"
     ]
    }
   ],
   "source": [
    "benchmarks = [(\"post-word2vec\", ft.partial(run_benchmark, model_str= 'post_word2vec')),\n",
    "             (\"orig-word2vec\", ft.partial(run_benchmark, model_str= 'orig_word2vec'))]\n",
    "\n",
    "pearson_results_year_task = {}\n",
    "\n",
    "for year_task in sts_all['year-task'].unique():\n",
    "    print('STS-' + year_task)\n",
    "    pearson_results_year_task['STS-' + year_task] = run_experiment(sts_by_year_task[year_task], benchmarks)  \n",
    "    \n",
    "pearson_results_year_task['SICK'] = run_experiment(sick_all, benchmarks) \n",
    "\n",
    "pearson_results_year_task['2015-answers_students'] = run_experiment(answers_students_2015, benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post-word2vec</th>\n",
       "      <th>orig-word2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STS-2012-MSRpar</th>\n",
       "      <td>34.42</td>\n",
       "      <td>41.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2012-MSRvid</th>\n",
       "      <td>79.63</td>\n",
       "      <td>76.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2012-surprise.OnWN</th>\n",
       "      <td>71.27</td>\n",
       "      <td>70.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2012-SMTeuroparl</th>\n",
       "      <td>40.32</td>\n",
       "      <td>31.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2012-surprise.SMTnews</th>\n",
       "      <td>50.09</td>\n",
       "      <td>51.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2013-FNWN</th>\n",
       "      <td>49.09</td>\n",
       "      <td>39.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2013-OnWN</th>\n",
       "      <td>75.57</td>\n",
       "      <td>67.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2013-headlines</th>\n",
       "      <td>63.65</td>\n",
       "      <td>63.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2014-OnWN</th>\n",
       "      <td>81.40</td>\n",
       "      <td>74.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2014-deft-forum</th>\n",
       "      <td>46.73</td>\n",
       "      <td>41.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2014-deft-news</th>\n",
       "      <td>67.88</td>\n",
       "      <td>66.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2014-headlines</th>\n",
       "      <td>60.93</td>\n",
       "      <td>60.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2014-tweet-news</th>\n",
       "      <td>76.00</td>\n",
       "      <td>73.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2014-images</th>\n",
       "      <td>80.55</td>\n",
       "      <td>77.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2015-answers-forums</th>\n",
       "      <td>66.77</td>\n",
       "      <td>52.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-answers_students</th>\n",
       "      <td>72.16</td>\n",
       "      <td>70.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2015-belief</th>\n",
       "      <td>77.08</td>\n",
       "      <td>60.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2015-headlines</th>\n",
       "      <td>69.02</td>\n",
       "      <td>68.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2015-images</th>\n",
       "      <td>83.08</td>\n",
       "      <td>80.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SICK</th>\n",
       "      <td>72.02</td>\n",
       "      <td>72.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           post-word2vec  orig-word2vec\n",
       "STS-2012-MSRpar                    34.42          41.78\n",
       "STS-2012-MSRvid                    79.63          76.27\n",
       "STS-2012-surprise.OnWN             71.27          70.62\n",
       "STS-2012-SMTeuroparl               40.32          31.20\n",
       "STS-2012-surprise.SMTnews          50.09          51.07\n",
       "STS-2013-FNWN                      49.09          39.68\n",
       "STS-2013-OnWN                      75.57          67.98\n",
       "STS-2013-headlines                 63.65          63.29\n",
       "STS-2014-OnWN                      81.40          74.85\n",
       "STS-2014-deft-forum                46.73          41.30\n",
       "STS-2014-deft-news                 67.88          66.76\n",
       "STS-2014-headlines                 60.93          60.87\n",
       "STS-2014-tweet-news                76.00          73.33\n",
       "STS-2014-images                    80.55          77.44\n",
       "STS-2015-answers-forums            66.77          52.65\n",
       "2015-answers_students              72.16          70.82\n",
       "STS-2015-belief                    77.08          60.11\n",
       "STS-2015-headlines                 69.02          68.11\n",
       "STS-2015-images                    83.08          80.07\n",
       "SICK                               72.02          72.25"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pearson_results_year_task_df = pd.DataFrame(pearson_results_year_task)\n",
    "pearson_results_year_task_df = pearson_results_year_task_df.transpose()\n",
    "pearson_results_year_task_df = pearson_results_year_task_df.rename(columns={i:b[0] for i, b in enumerate(benchmarks)})\n",
    "\n",
    "pearson_results_year_task_df.reindex(['STS-2012-MSRpar', 'STS-2012-MSRvid', 'STS-2012-surprise.OnWN', 'STS-2012-SMTeuroparl', 'STS-2012-surprise.SMTnews','STS-2013-FNWN', 'STS-2013-OnWN', 'STS-2013-headlines',  'STS-2014-OnWN', 'STS-2014-deft-forum','STS-2014-deft-news', 'STS-2014-headlines', 'STS-2014-tweet-news',  'STS-2014-images', 'STS-2015-answers-forums', '2015-answers_students', 'STS-2015-belief',  'STS-2015-headlines', 'STS-2015-images', 'SICK'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downstream task -- Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon Review\n",
    "AR_data = []\n",
    "\n",
    "with open('data/SentimentAnalysis/train_amazon_10000.csv', encoding = 'utf8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    headers = next(reader, None)\n",
    "    for row in reader:\n",
    "        AR_data.append([row[1],int(row[0])])\n",
    "        \n",
    "\n",
    "AR_label = []\n",
    "\n",
    "for i in range(0, len(AR_data)):\n",
    "    AR_label.append(int(AR_data[i][1]))\n",
    "    \n",
    "# Customer Review\n",
    "CR_data = []\n",
    "\n",
    "with open('data/SentimentAnalysis/custrev.neg', encoding = 'utf-8') as f:\n",
    "    for row in f:\n",
    "        CR_data.append([row,0])\n",
    "\n",
    "with open('data/SentimentAnalysis/custrev.pos', encoding = 'utf-8') as f:\n",
    "    for row in f:\n",
    "        CR_data.append([row,1])\n",
    "\n",
    "CR_label = []\n",
    "\n",
    "for i in range(0, len(CR_data)):\n",
    "    CR_label.append(int(CR_data[i][1]))\n",
    "    \n",
    "# IMDB\n",
    "IMDB_data = []\n",
    "\n",
    "with open('data/SentimentAnalysis/imdb_train_10000_new.csv', encoding = 'utf8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    #headers = next(reader, None)\n",
    "    for row in reader:        \n",
    "        s = 0\n",
    "        if row[0] == 'pos':\n",
    "            s = 1\n",
    "        IMDB_data.append([row[1],s])\n",
    "\n",
    "IMDB_label = []\n",
    "\n",
    "for i in range(0, len(IMDB_data)):\n",
    "    IMDB_label.append(int(IMDB_data[i][1]))\n",
    "\n",
    "# SST\n",
    "SST_data = []\n",
    "\n",
    "with open('data/SentimentAnalysis/sst_all.csv', encoding = 'utf-8-sig') as f:\n",
    "    reader = csv.reader(f)\n",
    "    #headers = next(reader, None)\n",
    "    for row in reader:        \n",
    "        SST_data.append([row[1],int(row[0])])\n",
    "\n",
    "SST_label = []\n",
    "\n",
    "for i in range(0, len(SST_data)):\n",
    "    SST_label.append(int(SST_data[i][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sentence_emb(sentence1, model_str): \n",
    "    model = eval(model_str)\n",
    "    \n",
    "    sentence1 = Sentence(sentence1)\n",
    "    \n",
    "    wv_len = 300\n",
    "    \n",
    "        \n",
    "    tokens1 =  sentence1.tokens\n",
    "    tokens1 = [token for token in tokens1 if token in model and token.islower()]\n",
    "    \n",
    "    if tokens1 == []:\n",
    "        embedding1 = np.zeros(wv_len)\n",
    "    elif tokens1 != []:   \n",
    "        embedding1 = np.average([model[token] for token in tokens1], axis=0)\n",
    "    \n",
    "    return embedding1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sent_emb_list(dataset, model_str):\n",
    "    data_list = []\n",
    "    for i in range(0, len(dataset)):\n",
    "        sent = dataset[i][0]\n",
    "    \n",
    "        data_list.append(convert_to_sentence_emb(sent, model_str))\n",
    "    \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR_word2vec = to_sent_emb_list(AR_data, 'post_word2vec')\n",
    "CR_word2vec = to_sent_emb_list(CR_data, 'post_word2vec')\n",
    "IMDB_word2vec = to_sent_emb_list(IMDB_data, 'post_word2vec')\n",
    "SST_word2vec = to_sent_emb_list(SST_data, 'post_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LR_crossval(model, label):\n",
    "    \n",
    "    LRClassifier = LogisticRegression(solver='sag',multi_class = 'multinomial')\n",
    "    LR_cv_results = cross_validate(LRClassifier, model, label, cv=5, return_train_score=True)\n",
    "    \n",
    "    return [np.mean(LR_cv_results['train_score']), np.mean(LR_cv_results['test_score'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AR:  [0.8466000771730482, 0.8376999831999958]\n",
      "CR:  [0.8246745350004291, 0.7823881871175774]\n",
      "IMDB:  [0.8474499646644527, 0.8433998841499711]\n",
      "SST:  [0.8240477182009938, 0.8056305235810155]\n"
     ]
    }
   ],
   "source": [
    "print('AR: ', LR_crossval(AR_word2vec, AR_label))\n",
    "print('CR: ', LR_crossval(CR_word2vec, CR_label))\n",
    "print('IMDB: ', LR_crossval(IMDB_word2vec, IMDB_label))\n",
    "print('SST: ', LR_crossval(SST_word2vec, SST_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_wv(word_vector_str):\n",
    "    \n",
    "    word_dictionary = eval(word_vector_str)\n",
    "    \n",
    "    ListWords = list(word_dictionary.keys())\n",
    "\n",
    "    print('writing to', 'HSR_RR_' + word_vector_str)\n",
    "\n",
    "    with open('small_HSR_RR_' + word_vector_str + '.txt', 'a', encoding = 'utf8') as the_file:\n",
    "        for word in ListWords:\n",
    "\n",
    "            wordVec = word_dictionary[word]\n",
    "            wordVecString = \" \".join(str(x) for x in wordVec)\n",
    "\n",
    "            the_file.write(word + ' ' + wordVecString  + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing to HSR_RR_post_word2vec\n"
     ]
    }
   ],
   "source": [
    "save_wv('post_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Code Causally Denoise Word Embeddings Using Half-Sibling Regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
